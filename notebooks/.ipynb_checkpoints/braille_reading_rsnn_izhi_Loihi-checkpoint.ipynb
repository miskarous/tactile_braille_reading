{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e320c66",
   "metadata": {},
   "source": [
    "## Notebook intended to run the working networks for Braille reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc3b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805050c",
   "metadata": {},
   "source": [
    "### Don't forget to select the threshold you want to work with and if you want to use pre-trained weights or train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61220963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "# multiple_gpus = True # set to 'True' if more than 1 GPU available\n",
    "use_nni_weights = False # set to 'True' for use of weights from NNI optimization \n",
    "use_seed = False # set seed to achive reproducable results\n",
    "run = \"_3\" # run number for statistics\n",
    "epochs = 300 # 300 # set the number of epochs you want to train the network here\n",
    "data_type = 0 # 0 is SA/SA, 1 is RA/RA, 2 is SA/RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3b60ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected. Running on CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    gpu_sel = 1\n",
    "    gpu_av = [torch.cuda.is_available() for ii in range(torch.cuda.device_count())]\n",
    "    print(\"Detected {} GPUs. The load will be shared.\".format(torch.cuda.device_count()))\n",
    "    if True in gpu_av:\n",
    "        if gpu_av[gpu_sel]:\n",
    "            device = torch.device(\"cuda:\"+str(gpu_sel))\n",
    "        else:\n",
    "            device = torch.device(\"cuda:\"+str(gpu_av.index(True)))\n",
    "        # torch.cuda.set_per_process_memory_fraction(0.25, device=device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Single GPU detected. Setting up the simulation there.\")\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e85c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data randomly\n"
     ]
    }
   ],
   "source": [
    "if use_seed:\n",
    "    seed = 42\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(\"Seed set to {}\".format(seed))\n",
    "else:\n",
    "    print(\"Shuffle data randomly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65739ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6509a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['Space', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f76b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace torch.tile() by torch_tile()\n",
    "def torch_tile(data, reps):\n",
    "    np_tile = np.tile(data.cpu().detach().numpy(),reps)\n",
    "    return torch.tensor(np_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef76430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract_augmented(params, file_name, taxels=None, letter_written=letters):\n",
    "    \n",
    "    max_time = int(52*25) # ms\n",
    "    time_bin_size = int(params['time_bin_size']) # ms\n",
    "    num_bins = round((max_time/time_bin_size)+0.5)\n",
    "    global time\n",
    "    time = range(0,max_time,time_bin_size)\n",
    "    \n",
    "    global time_step\n",
    "    time_step = time_bin_size*0.001\n",
    "    data_steps = len(time)\n",
    "    \n",
    "    infile = open(file_name, 'rb')\n",
    "    data_dict = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    labels = []\n",
    "    bins = 1000 # ms conversion\n",
    "    nchan = 12 # number of channels per sensor\n",
    "    selected_chans = 2*nchan\n",
    "\n",
    "    for i, trial in enumerate(data_dict):\n",
    "        dat_SA = trial['SA_stream']\n",
    "        dat_RA = trial['RA_stream']\n",
    "        events_array_SA = np.zeros([num_bins, nchan])\n",
    "        events_array_RA = np.zeros([num_bins, nchan])\n",
    "        for j in range(nchan):\n",
    "            for k in range(num_bins):\n",
    "                start_time = k*time_bin_size\n",
    "                end_time = ((k+1)*time_bin_size)\n",
    "                curr_bin_SA = dat_SA[j][start_time:end_time]\n",
    "                curr_bin_RA = dat_RA[j][start_time:end_time]\n",
    "                if any(curr_bin_SA):\n",
    "                    events_array_SA[k][j] = 1\n",
    "                if any(curr_bin_RA):\n",
    "                    events_array_RA[k][j] = 1\n",
    "        if (data_type == 0):\n",
    "            events_array_comb = np.concatenate((events_array_SA,events_array_SA),axis=1)\n",
    "        elif (data_type == 1):\n",
    "            events_array_comb = np.concatenate((events_array_RA,events_array_RA),axis=1)\n",
    "        else:\n",
    "            events_array_comb = np.concatenate((events_array_SA,events_array_RA),axis=1)\n",
    "        data.append(events_array_comb)\n",
    "        labels.append(letter_written.index(trial['letter']))\n",
    "        \n",
    "    # return data,labels\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    data = torch.tensor(data, dtype=dtype )    \n",
    "    labels = torch.tensor(labels,dtype=torch.long)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, shuffle=True, stratify=labels, random_state=42) # if fix seed wanted add: random_state=42\n",
    "\n",
    "    ds_train = TensorDataset(x_train,y_train)\n",
    "    ds_test = TensorDataset(x_test,y_test)\n",
    "    \n",
    "    return ds_train, ds_test, labels, selected_chans, data_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5dced6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn(inputs, layers):\n",
    "\n",
    "    bs = inputs.shape[0]\n",
    "    # h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs.tile((nb_input_copies,)), layers[0]))\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (torch_tile(inputs,(nb_input_copies,)).to(device), layers[0]))\n",
    "    syn = torch.zeros((bs,nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((bs,nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    out = torch.zeros((bs, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Here we define two lists which we use to record the membrane potentials and output spikes\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute hidden (recurrent) layer activity\n",
    "    for t in range(nb_steps):\n",
    "        h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, layers[2]))\n",
    "        \n",
    "        # LK: leak and integrate\n",
    "        new_syn = alpha*syn + h1\n",
    "        new_mem = beta*mem + new_syn\n",
    "        # new_mem = beta*mem + new_syn*(1 - spk_rec[-1]) if t != 0 else alpha*syn + h1\n",
    "\n",
    "        # LK: fire\n",
    "        mthr = new_mem-1.0\n",
    "        out = spike_fn(mthr)\n",
    "        rst = out.detach() # We do not want to backprop through the reset\n",
    "        \n",
    "        mem = new_mem*(1.0-rst)\n",
    "        syn = new_syn\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "\n",
    "    # Now we merge the recorded membrane potentials into a single tensor\n",
    "    mem_rec = torch.stack(mem_rec,dim=1)\n",
    "    spk_rec = torch.stack(spk_rec,dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2 = torch.einsum(\"abc,cd->abd\", (spk_rec, layers[1]))\n",
    "    flt = torch.zeros((bs,nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((bs,nb_outputs), device=device, dtype=dtype)\n",
    "    s_out_rec = [out] # out is initialized as zeros, so it is fine to start with this\n",
    "    out_rec = [out]\n",
    "    for t in range(nb_steps):        \n",
    "        # LK: leak and integrate\n",
    "        new_flt = alpha*flt + h2[:,t]\n",
    "        new_out = beta*out + new_flt\n",
    "            \n",
    "        # LK: fire\n",
    "        mthr_out = new_out-1.0\n",
    "        s_out = spike_fn(mthr_out)\n",
    "        rst_out = s_out.detach()\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out*(1.0-rst_out)\n",
    "\n",
    "        out_rec.append(out)\n",
    "        s_out_rec.append(s_out)\n",
    "\n",
    "    out_rec = torch.stack(out_rec,dim=1)\n",
    "    s_out_rec = torch.stack(s_out_rec,dim=1)\n",
    "    other_recs = [mem_rec, spk_rec, s_out_rec]\n",
    "    layers_update = layers\n",
    "\n",
    "    return out_rec, other_recs, layers_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a5862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layers(file, map_location, requires_grad=True, variable=False):\n",
    "    \n",
    "    if variable:\n",
    "        \n",
    "        lays = file\n",
    "        \n",
    "        for ii in lays:\n",
    "            ii.requires_grad = requires_grad\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        lays = torch.load(file, map_location=map_location)\n",
    "    \n",
    "        for ii in lays:\n",
    "            ii.requires_grad = requires_grad\n",
    "        \n",
    "    return lays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9357bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here, this function is only used to define the global variables to be used in other functions\n",
    "def build(params):\n",
    "    \n",
    "    global nb_input_copies\n",
    "    nb_input_copies = params['nb_input_copies']  # Num of spiking neurons used to encode each channel \n",
    "\n",
    "    # Network parameters\n",
    "    global nb_inputs\n",
    "    nb_inputs  = nb_channels*nb_input_copies\n",
    "    global nb_hidden\n",
    "    nb_hidden  = 450\n",
    "    global nb_outputs\n",
    "    nb_outputs = len(np.unique(labels))+1\n",
    "    global nb_steps\n",
    "    nb_steps = data_steps\n",
    "\n",
    "    tau_mem = params['tau_mem'] # ms\n",
    "    tau_syn = tau_mem/params['tau_ratio']\n",
    "    \n",
    "    global alpha\n",
    "    alpha   = float(np.exp(-time_step/tau_syn))\n",
    "    global beta\n",
    "    beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "    fwd_weight_scale = params['fwd_weight_scale']\n",
    "    rec_weight_scale = params['weight_scale_factor']*fwd_weight_scale \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24fedbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_predict(params, x, use_nni_weights):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    \n",
    "    global nb_input_copies\n",
    "    nb_input_copies = params['nb_input_copies']  # Num of spiking neurons used to encode each channel \n",
    "\n",
    "    # Network parameters\n",
    "    global nb_inputs\n",
    "    nb_inputs  = nb_channels*nb_input_copies\n",
    "    global nb_hidden\n",
    "    nb_hidden  = 450\n",
    "    global nb_outputs\n",
    "    nb_outputs = len(np.unique(labels))+1\n",
    "    global nb_steps\n",
    "    nb_steps = data_steps\n",
    "\n",
    "    tau_mem = params['tau_mem'] # ms\n",
    "    tau_syn = tau_mem/params['tau_ratio']\n",
    "    \n",
    "    global alpha\n",
    "    alpha   = float(np.exp(-time_step/tau_syn))\n",
    "    global beta\n",
    "    beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "    fwd_weight_scale = params['fwd_weight_scale']\n",
    "    rec_weight_scale = params['weight_scale_factor']*fwd_weight_scale \n",
    "    \n",
    "    # Spiking network\n",
    "    if use_nni_weights:\n",
    "        layers = load_layers('../NNI/SpyTorch_layers/best_test_'+file_type+'_thr_'+str(file_thr)+'_ref_'+str(file_ref)+'_'+optim_nni_experiment+'.pt', map_location=device)\n",
    "    else:\n",
    "        layers = []\n",
    "\n",
    "        w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.normal_(w1, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_inputs))\n",
    "        layers.append(w1)\n",
    "\n",
    "        w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.normal_(w2, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_hidden))\n",
    "        layers.append(w2)\n",
    "\n",
    "        v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.normal_(v1, mean=0.0, std=rec_weight_scale/np.sqrt(nb_hidden))\n",
    "        layers.append(v1)\n",
    "\n",
    "    \n",
    "    # Make predictions\n",
    "    output, _, _ = run_snn(x,layers)\n",
    "    m = torch.sum(others[-1],1) # sum over time\n",
    "    _, am = torch.max(m, 1)     # argmax over output units\n",
    "    \n",
    "    return letters[am.detach().cpu().numpy()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b13463e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, dataset, lr=0.0015, nb_epochs=300, opt_parameters=None, layers=None, dataset_test=None):\n",
    "    \n",
    "    ttc_hist = []\n",
    "    \n",
    "    if (opt_parameters != None) & (layers != None):\n",
    "        parameters = opt_parameters # The paramters we want to optimize\n",
    "        layers = layers\n",
    "    elif (opt_parameters != None) & (layers == None): \n",
    "        parameters = opt_parameters\n",
    "        layers = [w1,w2,v1]\n",
    "    elif (opt_parameters == None) & (layers != None):\n",
    "        parameters = [w1,w2,v1]\n",
    "        layers = layers\n",
    "    elif (opt_parameters == None) & (layers == None): # default from tutorial 5\n",
    "        parameters = [w1,w2,v1]\n",
    "        layers = [w1,w2,v1]\n",
    "        \n",
    "    optimizer = torch.optim.Adamax(parameters, lr=0.0015, betas=(0.9,0.995)) # params['lr']\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1) # The log softmax function across output units\n",
    "    loss_fn = nn.NLLLoss() # The negative log likelihood loss function\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    # The optimization loop\n",
    "    loss_hist = []\n",
    "    accs_hist = [[],[]]\n",
    "    for e in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        # accs: mean training accuracies for each batch\n",
    "        accs = []\n",
    "        for x_local, y_local in generator:\n",
    "            x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "            output,recs,layers_update = run_snn(x_local,layers)\n",
    "            _,spks,_=recs\n",
    "            # with output spikes\n",
    "            m = torch.sum(recs[-1],1) # sum over time\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "        \n",
    "            # Here we can set up our regularizer loss\n",
    "            reg_loss = params['reg_spikes']*torch.mean(torch.sum(spks,1)) # e.g., L1 loss on total number of spikes (original: 1e-3)\n",
    "            reg_loss += params['reg_neurons']*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron (original: 2e-6)\n",
    "        \n",
    "            # Here we combine supervised loss and the regularizer\n",
    "            loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "        \n",
    "            # compare to labels\n",
    "            _, am = torch.max(m, 1) # argmax over output units\n",
    "            tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "            accs.append(tmp)\n",
    "    \n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        \n",
    "        # mean_accs: mean training accuracy of current epoch (average over all batches)\n",
    "        mean_accs = np.mean(accs)\n",
    "        accs_hist[0].append(mean_accs)\n",
    "\n",
    "        # Calculate test accuracy in each epoch\n",
    "        if dataset_test is not None:\n",
    "            test_acc, test_ttc = compute_classification_accuracy(\n",
    "                params,\n",
    "                dataset_test,\n",
    "                layers=layers_update,\n",
    "                early=True\n",
    "            )\n",
    "            accs_hist[1].append(test_acc) # only safe best test\n",
    "            ttc_hist.append(test_ttc)\n",
    "        \n",
    "        if dataset_test is None:\n",
    "            # save best training\n",
    "            if mean_accs >= np.max(accs_hist[0]):\n",
    "                best_acc_layers = []\n",
    "                for ii in layers_update:\n",
    "                    best_acc_layers.append(ii.detach().clone())\n",
    "        else:\n",
    "            # save best test\n",
    "            if np.max(test_acc) >= np.max(accs_hist[1]):\n",
    "                best_acc_layers = []\n",
    "                for ii in layers_update:\n",
    "                    best_acc_layers.append(ii.detach().clone())\n",
    "        \n",
    "        print(\"Epoch {}/{} done. Train accuracy: {:.2f}%, Test accuracy: {:.2f}%.\".format(e+1,nb_epochs, accs_hist[0][-1]*100, accs_hist[1][-1]*100))\n",
    "    \n",
    "\n",
    "    return loss_hist, accs_hist, best_acc_layers, ttc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c1b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(params, ds_train, ds_test, epochs=epochs):\n",
    "    \n",
    "    global nb_input_copies\n",
    "    nb_input_copies = params['nb_input_copies']  # Num of spiking neurons used to encode each channel \n",
    "\n",
    "    # Network parameters\n",
    "    global nb_inputs\n",
    "    nb_inputs  = nb_channels*nb_input_copies\n",
    "    global nb_hidden\n",
    "    nb_hidden  = 450\n",
    "    global nb_outputs\n",
    "    nb_outputs = len(np.unique(labels))+1\n",
    "    global nb_steps\n",
    "    nb_steps = data_steps\n",
    "\n",
    "    tau_mem = params['tau_mem'] # ms\n",
    "    tau_syn = tau_mem/params['tau_ratio']\n",
    "    \n",
    "    global alpha\n",
    "    alpha   = float(np.exp(-time_step/tau_syn))\n",
    "    global beta\n",
    "    beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "    fwd_weight_scale = params['fwd_weight_scale']\n",
    "    rec_weight_scale = params['weight_scale_factor']*fwd_weight_scale\n",
    "\n",
    "    # Spiking network\n",
    "    layers = []\n",
    "    w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w1, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_inputs))\n",
    "    layers.append(w1)\n",
    "\n",
    "    w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w2, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_hidden))\n",
    "    layers.append(w2)\n",
    "    \n",
    "    v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(v1, mean=0.0, std=rec_weight_scale/np.sqrt(nb_hidden))\n",
    "    layers.append(v1)\n",
    "\n",
    "    layers_init = []\n",
    "    for ii in layers:\n",
    "        layers_init.append(ii.detach().clone())\n",
    "\n",
    "    opt_parameters = [w1, w2, v1]\n",
    "    \n",
    "    # a fixed learning rate is already defined within the train function, that's why here it is omitted\n",
    "    loss_hist, accs_hist, best_layers, _ = train(params, ds_train, nb_epochs=epochs, opt_parameters=opt_parameters, layers=layers, dataset_test=ds_test)\n",
    "\n",
    "    # best training and test at best training\n",
    "    acc_best_train = np.max(accs_hist[0]) # returns max value\n",
    "    acc_best_train = acc_best_train*100\n",
    "    idx_best_train = np.argmax(accs_hist[0]) # returns index of max value \n",
    "    acc_test_at_best_train = accs_hist[1][idx_best_train]*100\n",
    "\n",
    "    # best test and training at best test\n",
    "    acc_best_test = np.max(accs_hist[1])\n",
    "    acc_best_test = acc_best_test*100\n",
    "    idx_best_test = np.argmax(accs_hist[1])\n",
    "    acc_train_at_best_test = accs_hist[0][idx_best_test]*100\n",
    "\n",
    "    print(\"Final results: \\n\")\n",
    "    print(\"Best training accuracy: {:.2f}% and according test accuracy: {:.2f}% at epoch: {}\".format(acc_best_train, acc_test_at_best_train, idx_best_train+1)) # only from training\n",
    "    print(\"Best test accuracy: {:.2f}% and according train accuracy: {:.2f}% at epoch: {}\".format(acc_best_test, acc_train_at_best_test, idx_best_test+1)) # only from training\n",
    "\n",
    "    return loss_hist, accs_hist, best_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e8ca94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy(params, dataset, layers=None, early=False):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=128,\n",
    "                           shuffle=False, num_workers=2)\n",
    "    accs = []\n",
    "    multi_accs = []\n",
    "    ttc = None\n",
    "\n",
    "    for x_local, y_local in generator:\n",
    "        x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "        if layers == None:\n",
    "            layers = [w1,w2,v1]\n",
    "            output, others, _= run_snn(x_local,layers)\n",
    "        else:\n",
    "            output, others, _ = run_snn(x_local,layers)\n",
    "        # with output spikes\n",
    "        m = torch.sum(others[-1],1) # sum over time\n",
    "        _, am = torch.max(m, 1)     # argmax over output units\n",
    "        # compare to labels\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "        accs.append(tmp)\n",
    "\n",
    "        if early:\n",
    "            accs_early = []\n",
    "            for t in range(output.shape[1]-1):\n",
    "                # with spiking output layer\n",
    "                m_early = torch.sum(others[-1][:,:t+1,:],1) # sum over time\n",
    "                _, am_early = torch.max(m_early, 1)         # argmax over output units\n",
    "                # compare to labels\n",
    "                tmp_early = np.mean((y_local == am_early).detach().cpu().numpy())\n",
    "                accs_early.append(tmp_early)\n",
    "            multi_accs.append(accs_early)\n",
    "    \n",
    "    if early:\n",
    "        mean_multi = np.mean(multi_accs, axis=0)\n",
    "        if np.max(mean_multi) > mean_multi[-1]:\n",
    "            if mean_multi[-2]==mean_multi[-1]:\n",
    "                flattening = []\n",
    "                for ii in range(len(mean_multi)-2,1,-1):\n",
    "                    if mean_multi[ii] != mean_multi[ii-1]:\n",
    "                        flattening.append(ii)\n",
    "                # time to classify\n",
    "                ttc = time[flattening[0]]\n",
    "            else:\n",
    "                # time to classify\n",
    "                ttc = time[-1]\n",
    "        else:\n",
    "            # time to classify\n",
    "            ttc = time[np.argmax(mean_multi)]\n",
    "\n",
    "    return np.mean(accs), ttc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fda850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(params, dataset, save, layers=None, labels=letters):\n",
    "    \n",
    "    generator = DataLoader(dataset, batch_size=128,\n",
    "                           shuffle=False, num_workers=2)\n",
    "    accs = []\n",
    "    multi_accs = []\n",
    "    trues = []\n",
    "    preds = []\n",
    "    for x_local, y_local in generator:\n",
    "        x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "        if layers == None:\n",
    "            layers = [w1, w2, v1]\n",
    "            output, others, _= run_snn(x_local, layers)\n",
    "        else:\n",
    "            output, others, _= run_snn(x_local, layers)\n",
    "        # with output spikes\n",
    "        m = torch.sum(others[-1],1) # sum over time\n",
    "        _, am = torch.max(m, 1)     # argmax over output units\n",
    "        # compare to labels\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "        accs.append(tmp)\n",
    "        trues.extend(y_local.detach().cpu().numpy())\n",
    "        preds.extend(am.detach().cpu().numpy())\n",
    "    \n",
    "    #return trues, preds\n",
    "    \n",
    "    cm = confusion_matrix(trues, preds, normalize='true')\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "    plt.figure(figsize=(12,9))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.1g',\n",
    "               #linewidths=0.005,\n",
    "               #linecolor='black',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46e6a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir_data = './data/'\n",
    "file_type = 'data_braille_letters_izhi'\n",
    "file_type_orig = 'data_braille_letters_th1'\n",
    "file_name = file_dir_data + file_type\n",
    "\n",
    "file_dir_params = '../parameters/'\n",
    "param_filename = 'parameters_th1.txt'\n",
    "file_name_parameters = file_dir_params + param_filename\n",
    "params = {}\n",
    "with open(file_name_parameters) as file:\n",
    " for line in file:\n",
    "    (key, value) = line.split()\n",
    "    if key == 'time_bin_size' or key == 'nb_input_copies':\n",
    "        params[key] = int(value)\n",
    "    else:\n",
    "        params[key] = np.double(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35f2739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements \n",
    "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid \n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = params['scale']\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which \n",
    "        we need to later backpropagate our error signals. To achieve this we use the \n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the \n",
    "        surrogate gradient of the loss with respect to the input. \n",
    "        Here we use the normalized negative part of a fast sigmoid \n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30184b8a",
   "metadata": {},
   "source": [
    "### Train and test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b62e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 done. Train accuracy: 14.46%, Test accuracy: 24.73%.\n",
      "Epoch 2/300 done. Train accuracy: 32.15%, Test accuracy: 38.24%.\n",
      "Epoch 3/300 done. Train accuracy: 45.48%, Test accuracy: 48.76%.\n",
      "Epoch 4/300 done. Train accuracy: 52.08%, Test accuracy: 49.93%.\n",
      "Epoch 5/300 done. Train accuracy: 48.26%, Test accuracy: 40.58%.\n",
      "Epoch 6/300 done. Train accuracy: 47.69%, Test accuracy: 51.49%.\n",
      "Epoch 7/300 done. Train accuracy: 50.01%, Test accuracy: 46.65%.\n",
      "Epoch 8/300 done. Train accuracy: 42.35%, Test accuracy: 41.64%.\n",
      "Epoch 9/300 done. Train accuracy: 46.29%, Test accuracy: 49.21%.\n",
      "Epoch 10/300 done. Train accuracy: 51.91%, Test accuracy: 54.46%.\n",
      "Epoch 11/300 done. Train accuracy: 54.92%, Test accuracy: 54.46%.\n",
      "Epoch 12/300 done. Train accuracy: 55.61%, Test accuracy: 52.53%.\n",
      "Epoch 13/300 done. Train accuracy: 50.88%, Test accuracy: 54.84%.\n",
      "Epoch 14/300 done. Train accuracy: 51.68%, Test accuracy: 48.09%.\n",
      "Epoch 15/300 done. Train accuracy: 51.16%, Test accuracy: 47.47%.\n",
      "Epoch 16/300 done. Train accuracy: 49.92%, Test accuracy: 36.84%.\n",
      "Epoch 17/300 done. Train accuracy: 44.20%, Test accuracy: 48.57%.\n",
      "Epoch 18/300 done. Train accuracy: 54.14%, Test accuracy: 55.00%.\n",
      "Epoch 19/300 done. Train accuracy: 53.08%, Test accuracy: 44.36%.\n",
      "Epoch 20/300 done. Train accuracy: 42.04%, Test accuracy: 32.28%.\n",
      "Epoch 21/300 done. Train accuracy: 39.25%, Test accuracy: 41.78%.\n",
      "Epoch 22/300 done. Train accuracy: 49.96%, Test accuracy: 49.02%.\n",
      "Epoch 23/300 done. Train accuracy: 49.13%, Test accuracy: 52.57%.\n",
      "Epoch 24/300 done. Train accuracy: 42.97%, Test accuracy: 24.37%.\n",
      "Epoch 25/300 done. Train accuracy: 30.59%, Test accuracy: 40.58%.\n",
      "Epoch 26/300 done. Train accuracy: 38.03%, Test accuracy: 40.96%.\n",
      "Epoch 27/300 done. Train accuracy: 42.85%, Test accuracy: 45.34%.\n",
      "Epoch 28/300 done. Train accuracy: 50.54%, Test accuracy: 51.10%.\n",
      "Epoch 29/300 done. Train accuracy: 53.49%, Test accuracy: 50.22%.\n",
      "Epoch 30/300 done. Train accuracy: 49.06%, Test accuracy: 43.68%.\n",
      "Epoch 31/300 done. Train accuracy: 33.98%, Test accuracy: 29.84%.\n",
      "Epoch 32/300 done. Train accuracy: 30.93%, Test accuracy: 30.75%.\n",
      "Epoch 33/300 done. Train accuracy: 32.26%, Test accuracy: 27.43%.\n",
      "Epoch 34/300 done. Train accuracy: 22.06%, Test accuracy: 22.71%.\n",
      "Epoch 35/300 done. Train accuracy: 24.82%, Test accuracy: 23.95%.\n",
      "Epoch 36/300 done. Train accuracy: 28.38%, Test accuracy: 34.98%.\n",
      "Epoch 37/300 done. Train accuracy: 28.97%, Test accuracy: 31.13%.\n",
      "Epoch 38/300 done. Train accuracy: 35.36%, Test accuracy: 39.05%.\n",
      "Epoch 39/300 done. Train accuracy: 40.20%, Test accuracy: 40.53%.\n",
      "Epoch 40/300 done. Train accuracy: 42.81%, Test accuracy: 36.02%.\n",
      "Epoch 41/300 done. Train accuracy: 39.73%, Test accuracy: 37.91%.\n",
      "Epoch 42/300 done. Train accuracy: 35.17%, Test accuracy: 27.74%.\n",
      "Epoch 43/300 done. Train accuracy: 31.27%, Test accuracy: 31.26%.\n",
      "Epoch 44/300 done. Train accuracy: 35.45%, Test accuracy: 42.86%.\n",
      "Epoch 45/300 done. Train accuracy: 45.47%, Test accuracy: 39.92%.\n",
      "Epoch 46/300 done. Train accuracy: 37.19%, Test accuracy: 37.21%.\n",
      "Epoch 47/300 done. Train accuracy: 40.41%, Test accuracy: 37.69%.\n",
      "Epoch 48/300 done. Train accuracy: 37.01%, Test accuracy: 36.43%.\n",
      "Epoch 49/300 done. Train accuracy: 36.47%, Test accuracy: 40.76%.\n",
      "Epoch 50/300 done. Train accuracy: 40.39%, Test accuracy: 39.37%.\n",
      "Epoch 51/300 done. Train accuracy: 43.01%, Test accuracy: 40.85%.\n",
      "Epoch 52/300 done. Train accuracy: 43.78%, Test accuracy: 42.01%.\n",
      "Epoch 53/300 done. Train accuracy: 46.84%, Test accuracy: 46.86%.\n",
      "Epoch 54/300 done. Train accuracy: 48.34%, Test accuracy: 48.33%.\n",
      "Epoch 55/300 done. Train accuracy: 49.00%, Test accuracy: 48.12%.\n",
      "Epoch 56/300 done. Train accuracy: 50.65%, Test accuracy: 47.51%.\n",
      "Epoch 57/300 done. Train accuracy: 51.51%, Test accuracy: 48.87%.\n",
      "Epoch 58/300 done. Train accuracy: 51.32%, Test accuracy: 47.07%.\n",
      "Epoch 59/300 done. Train accuracy: 51.54%, Test accuracy: 51.00%.\n",
      "Epoch 60/300 done. Train accuracy: 50.84%, Test accuracy: 50.52%.\n",
      "Epoch 61/300 done. Train accuracy: 50.16%, Test accuracy: 48.00%.\n",
      "Epoch 62/300 done. Train accuracy: 50.18%, Test accuracy: 46.60%.\n",
      "Epoch 63/300 done. Train accuracy: 47.32%, Test accuracy: 43.33%.\n",
      "Epoch 64/300 done. Train accuracy: 47.07%, Test accuracy: 44.32%.\n",
      "Epoch 65/300 done. Train accuracy: 47.37%, Test accuracy: 47.79%.\n",
      "Epoch 66/300 done. Train accuracy: 48.35%, Test accuracy: 43.42%.\n",
      "Epoch 67/300 done. Train accuracy: 48.70%, Test accuracy: 49.55%.\n",
      "Epoch 68/300 done. Train accuracy: 51.50%, Test accuracy: 49.23%.\n",
      "Epoch 69/300 done. Train accuracy: 51.00%, Test accuracy: 48.81%.\n",
      "Epoch 70/300 done. Train accuracy: 53.98%, Test accuracy: 47.28%.\n",
      "Epoch 71/300 done. Train accuracy: 52.50%, Test accuracy: 46.79%.\n",
      "Epoch 72/300 done. Train accuracy: 48.97%, Test accuracy: 43.43%.\n",
      "Epoch 73/300 done. Train accuracy: 46.35%, Test accuracy: 52.54%.\n",
      "Epoch 74/300 done. Train accuracy: 56.29%, Test accuracy: 53.71%.\n",
      "Epoch 75/300 done. Train accuracy: 55.93%, Test accuracy: 50.22%.\n",
      "Epoch 76/300 done. Train accuracy: 52.03%, Test accuracy: 52.26%.\n",
      "Epoch 77/300 done. Train accuracy: 52.99%, Test accuracy: 50.41%.\n",
      "Epoch 78/300 done. Train accuracy: 54.26%, Test accuracy: 53.50%.\n",
      "Epoch 79/300 done. Train accuracy: 54.65%, Test accuracy: 53.32%.\n",
      "Epoch 80/300 done. Train accuracy: 54.26%, Test accuracy: 55.94%.\n",
      "Epoch 81/300 done. Train accuracy: 54.99%, Test accuracy: 53.25%.\n",
      "Epoch 82/300 done. Train accuracy: 56.10%, Test accuracy: 54.44%.\n",
      "Epoch 83/300 done. Train accuracy: 56.63%, Test accuracy: 52.91%.\n",
      "Epoch 84/300 done. Train accuracy: 57.21%, Test accuracy: 54.28%.\n",
      "Epoch 85/300 done. Train accuracy: 56.75%, Test accuracy: 54.14%.\n",
      "Epoch 86/300 done. Train accuracy: 56.82%, Test accuracy: 52.18%.\n",
      "Epoch 87/300 done. Train accuracy: 53.55%, Test accuracy: 51.91%.\n",
      "Epoch 88/300 done. Train accuracy: 54.81%, Test accuracy: 52.12%.\n",
      "Epoch 89/300 done. Train accuracy: 54.64%, Test accuracy: 51.36%.\n",
      "Epoch 90/300 done. Train accuracy: 54.67%, Test accuracy: 49.81%.\n",
      "Epoch 91/300 done. Train accuracy: 54.62%, Test accuracy: 53.94%.\n",
      "Epoch 92/300 done. Train accuracy: 56.74%, Test accuracy: 55.65%.\n",
      "Epoch 93/300 done. Train accuracy: 59.41%, Test accuracy: 55.67%.\n",
      "Epoch 94/300 done. Train accuracy: 59.79%, Test accuracy: 55.96%.\n",
      "Epoch 95/300 done. Train accuracy: 59.27%, Test accuracy: 56.31%.\n",
      "Epoch 96/300 done. Train accuracy: 58.25%, Test accuracy: 54.17%.\n",
      "Epoch 97/300 done. Train accuracy: 55.00%, Test accuracy: 51.17%.\n",
      "Epoch 98/300 done. Train accuracy: 56.48%, Test accuracy: 56.55%.\n"
     ]
    }
   ],
   "source": [
    "if not use_nni_weights:\n",
    "    ds_train, ds_test, labels, nb_channels, data_steps = load_and_extract_augmented(params, file_name, letter_written=letters)\n",
    "\n",
    "    loss_hist, acc_hist, best_layers = build_and_train(params, ds_train, ds_test, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6b2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not use_nni_weights:\n",
    "    plt.plot(range(1,len(acc_hist[0])+1),100*np.array(acc_hist[0]), color='blue')\n",
    "    plt.plot(range(1,len(acc_hist[1])+1),100*np.array(acc_hist[1]), color='orange')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend([\"Training\",\"Test\"], loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b517b21",
   "metadata": {},
   "source": [
    "### Test the pre-trained network if you already have the pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_nni_weights:\n",
    "    # saved from NNI:\n",
    "    layers = load_layers(\"weights/SpyTorch_trained_weights_rec_th\" + file_thr + run + \".pt\", map_location=device)\n",
    "\n",
    "    print(\"Input weights matrix: {}x{}\".format(len(layers[0]),len(layers[0][0])))\n",
    "    print(\"Hidden weights matrix: {}x{}\".format(len(layers[2]),len(layers[2][0])))\n",
    "    print(\"Output weights matrix: {}x{}\".format(len(layers[1]),len(layers[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_nni_weights:\n",
    "    ds_train, ds_test, labels, nb_channels, data_steps = load_and_extract_augmented(params, file_name, letter_written=letters)\n",
    "\n",
    "    build(params)\n",
    "\n",
    "    test_acc = compute_classification_accuracy(params, ds_test, layers=layers, early=True)\n",
    "\n",
    "    print(\"Test accuracy: {}%\".format(np.round(test_acc[0]*100,2)))\n",
    "    print(\"Test accuracy as it comes, without rounding: {}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69056f",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "if use_nni_weights:\n",
    "    # from SAVED layers (from NNI) corresponding to best test:\n",
    "    ConfusionMatrix(params, ds_test, layers=load_layers(\"weights/SpyTorch_trained_weights_rec_th\" + file_thr + \".pt\", map_location=device), save=save)\n",
    "else:\n",
    "    # from the JUST TRAINED layers corresponding to best test:\n",
    "    ConfusionMatrix(params, ds_test, layers=best_layers, save=save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed526e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_layers, \"weights/SpyTorch_trained_weights_rec_th\" + file_thr + run + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9252bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
