{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc1e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4eaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "use_seed = False\n",
    "epochs = 300 # set the number of epochs you want to train the network (default = 300)\n",
    "save_fig = False # set True to save the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a577380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    path = '../plots'\n",
    "    isExist = os.path.exists(path)\n",
    "\n",
    "    if not isExist:\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ff6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected. Running on CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    gpu_sel = 1\n",
    "    gpu_av = [torch.cuda.is_available() for ii in range(torch.cuda.device_count())]\n",
    "    print(\"Detected {} GPUs. The load will be shared.\".format(torch.cuda.device_count()))\n",
    "    for gpu in range(len(gpu_av)):\n",
    "        if True in gpu_av:\n",
    "            if gpu_av[gpu_sel]:\n",
    "                device = torch.device(\"cuda:\"+str(gpu))\n",
    "                torch.cuda.set_per_process_memory_fraction(0.5, device=device)\n",
    "                print(\"Selected GPUs: {}\" .format(\"cuda:\"+str(gpu)))\n",
    "            else:\n",
    "                device = torch.device(\"cuda:\"+str(gpu_av.index(True)))\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"No GPU detected. Running on CPU.\")\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Single GPU detected. Setting up the simulation there.\")\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        torch.cuda.set_per_process_memory_fraction(0.5, device=device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2291f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data randomly\n"
     ]
    }
   ],
   "source": [
    "if use_seed:\n",
    "    seed = 42\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(\"Seed set to {}\".format(seed))\n",
    "else:\n",
    "    print(\"Shuffle data randomly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a7d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03bcccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['Space', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ad5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract(params, file_name, taxels=None, letter_written=letters):\n",
    "    \n",
    "    max_time = int(52*25) # ms\n",
    "    time_bin_size = int(params['time_bin_size']) # ms\n",
    "    num_bins = round((max_time/time_bin_size)+0.5)\n",
    "    global time\n",
    "    time = range(0,max_time,time_bin_size)\n",
    "    \n",
    "    global time_step\n",
    "    time_step = time_bin_size*0.001\n",
    "    data_steps = len(time)\n",
    "    \n",
    "    infile = open(file_name, 'rb')\n",
    "    data_dict = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    labels = []\n",
    "    bins = 1000 # ms conversion\n",
    "    nchan = 12 # number of channels per sensor\n",
    "    selected_chans = 2*nchan\n",
    "\n",
    "    for i, trial in enumerate(data_dict):\n",
    "        dat_SA = trial['SA_stream']\n",
    "        dat_RA = trial['RA_stream']\n",
    "        events_array_SA = np.zeros([num_bins, nchan])\n",
    "        events_array_RA = np.zeros([num_bins, nchan])\n",
    "        for j in range(nchan):\n",
    "            for k in range(num_bins):\n",
    "                start_time = k*time_bin_size\n",
    "                end_time = ((k+1)*time_bin_size)\n",
    "                curr_bin_SA = dat_SA[j][start_time:end_time]\n",
    "                curr_bin_RA = dat_RA[j][start_time:end_time]\n",
    "                if any(curr_bin_SA):\n",
    "                    events_array_SA[k][j] = 1\n",
    "                if any(curr_bin_RA):\n",
    "                    events_array_RA[k][j] = 1\n",
    "        events_array_comb = np.concatenate((events_array_SA,events_array_RA),axis=1)\n",
    "        data.append(events_array_comb)\n",
    "        labels.append(letter_written.index(trial['letter']))\n",
    "        \n",
    "    # return data,labels\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    data = torch.tensor(data, dtype=dtype )    \n",
    "    labels = torch.tensor(labels,dtype=torch.long)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, shuffle=True, stratify=labels)\n",
    "\n",
    "    ds_train = TensorDataset(x_train,y_train)\n",
    "    ds_test = TensorDataset(x_test,y_test)\n",
    "    \n",
    "    return ds_train, ds_test, labels, selected_chans, data_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebfd634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn(inputs, layers):\n",
    "\n",
    "    bs = inputs.shape[0]\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs.tile((nb_input_copies,)), layers[0]))\n",
    "    syn = torch.zeros((bs,nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((bs,nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    out = torch.zeros((bs, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Here we define two lists which we use to record the membrane potentials and output spikes\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute recurrent layer activity\n",
    "    for t in range(nb_steps):\n",
    "        h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, layers[2]))\n",
    "        mthr = mem-1.0\n",
    "        out = spike_fn(mthr)\n",
    "        rst = out.detach() # We do not want to backprop through the reset\n",
    "\n",
    "        new_syn = alpha*syn + h1\n",
    "        new_mem = (beta*mem + syn)*(1.0-rst)\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "    \n",
    "        mem = new_mem\n",
    "        syn = new_syn\n",
    "\n",
    "    # Now we merge the recorded membrane potentials into a single tensor\n",
    "    mem_rec = torch.stack(mem_rec,dim=1)\n",
    "    spk_rec = torch.stack(spk_rec,dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2 = torch.einsum(\"abc,cd->abd\", (spk_rec, layers[1]))\n",
    "    flt = torch.zeros((bs,nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((bs,nb_outputs), device=device, dtype=dtype)\n",
    "    s_out_rec = [out]\n",
    "    out_rec = [out]\n",
    "    for t in range(nb_steps):\n",
    "        mthr_out = out-1.0\n",
    "        s_out = spike_fn(mthr_out)\n",
    "        rst_out = s_out.detach()\n",
    "\n",
    "        new_flt = alpha*flt + h2[:,t]\n",
    "        new_out = (beta*out + flt)*(1.0-rst_out)\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "\n",
    "        out_rec.append(out)\n",
    "        s_out_rec.append(s_out)\n",
    "\n",
    "    out_rec = torch.stack(out_rec,dim=1)\n",
    "    s_out_rec = torch.stack(s_out_rec,dim=1)\n",
    "    other_recs = [mem_rec, spk_rec, s_out_rec]\n",
    "    layers_update = layers\n",
    "\n",
    "    \n",
    "    return out_rec, other_recs, layers_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4b7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layers(file, map_location, requires_grad=True, variable=False):\n",
    "    \n",
    "    if variable:\n",
    "        \n",
    "        lays = file\n",
    "        \n",
    "        for ii in lays:\n",
    "            ii.requires_grad = requires_grad\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        lays = torch.load(file, map_location=map_location)\n",
    "    \n",
    "        for ii in lays:\n",
    "            ii.requires_grad = requires_grad\n",
    "        \n",
    "    return lays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99bf0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_predict(params, x):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    \n",
    "    global nb_input_copies\n",
    "    nb_input_copies = params['nb_input_copies']  # Num of spiking neurons used to encode each channel \n",
    "\n",
    "    # Network parameters\n",
    "    global nb_inputs\n",
    "    nb_inputs  = nb_channels*nb_input_copies\n",
    "    global nb_hidden\n",
    "    nb_hidden  = 450\n",
    "    global nb_outputs\n",
    "    nb_outputs = len(np.unique(labels))+1\n",
    "    global nb_steps\n",
    "    nb_steps = data_steps\n",
    "\n",
    "    tau_mem = params['tau_mem'] # ms\n",
    "    tau_syn = tau_mem/params['tau_ratio']\n",
    "    \n",
    "    global alpha\n",
    "    alpha   = float(np.exp(-time_step/tau_syn))\n",
    "    global beta\n",
    "    beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "    fwd_weight_scale = params['fwd_weight_scale']\n",
    "    rec_weight_scale = params['weight_scale_factor']*fwd_weight_scale \n",
    "    \n",
    "    # Spiking network\n",
    "    layers = []\n",
    "\n",
    "    w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w1, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_inputs))\n",
    "    layers.append(w1)\n",
    "\n",
    "    w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w2, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_hidden))\n",
    "    layers.append(w2)\n",
    "\n",
    "    v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(v1, mean=0.0, std=rec_weight_scale/np.sqrt(nb_hidden))\n",
    "    layers.append(v1)\n",
    "\n",
    "    \n",
    "    # Make predictions\n",
    "    output, _, _ = run_snn(x,layers)\n",
    "    m = torch.sum(others[-1],1) # sum over time\n",
    "    _, am = torch.max(m, 1)     # argmax over output units\n",
    "    \n",
    "    return letters[am.detach().cpu().numpy()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d208c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, dataset, lr=0.0015, nb_epochs=300, opt_parameters=None, layers=None, dataset_test=None):\n",
    "    \n",
    "    ttc_hist = []\n",
    "    \n",
    "    if (opt_parameters != None) & (layers != None):\n",
    "        parameters = opt_parameters\n",
    "        layers = layers\n",
    "    elif (opt_parameters != None) & (layers == None): \n",
    "        parameters = opt_parameters\n",
    "        layers = [w1,w2,v1]\n",
    "    elif (opt_parameters == None) & (layers != None):\n",
    "        parameters = [w1,w2,v1]\n",
    "        layers = layers\n",
    "    elif (opt_parameters == None) & (layers == None):\n",
    "        parameters = [w1,w2,v1]\n",
    "        layers = [w1,w2,v1]\n",
    "        \n",
    "    optimizer = torch.optim.Adamax(parameters, lr=0.0015, betas=(0.9,0.995))\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1) # The log softmax function across output units\n",
    "    loss_fn = nn.NLLLoss() # The negative log likelihood loss function\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    # The optimization loop\n",
    "    loss_hist = []\n",
    "    accs_hist = [[],[]]\n",
    "    for e in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        # accs: mean training accuracies for each batch\n",
    "        accs = []\n",
    "        for x_local, y_local in generator:\n",
    "            x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "            output,recs,layers_update = run_snn(x_local,layers)\n",
    "            _,spks,_=recs\n",
    "            \n",
    "            m = torch.sum(recs[-1],1) # sum over time\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "        \n",
    "            # Here we can set up our regularizer loss\n",
    "            reg_loss = params['reg_spikes']*torch.mean(torch.sum(spks,1)) # L1 loss on total number of spikes\n",
    "            reg_loss += params['reg_neurons']*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
    "        \n",
    "            # Here we combine supervised loss and the regularizer\n",
    "            loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "        \n",
    "            # compare to labels\n",
    "            _, am = torch.max(m, 1) # argmax over output units\n",
    "            tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "            accs.append(tmp)\n",
    "    \n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        \n",
    "        # mean_accs: mean training accuracy of current epoch (average over all batches)\n",
    "        mean_accs = np.mean(accs)\n",
    "        accs_hist[0].append(mean_accs)\n",
    "\n",
    "        # Calculate test accuracy in each epoch\n",
    "        if dataset_test is not None:\n",
    "            test_acc, test_ttc = compute_classification_accuracy(\n",
    "                params,\n",
    "                dataset_test,\n",
    "                layers=layers_update,\n",
    "                early=True\n",
    "            )\n",
    "            accs_hist[1].append(test_acc)\n",
    "            ttc_hist.append(test_ttc)\n",
    "        \n",
    "        if dataset_test is None:\n",
    "            # save best training\n",
    "            if mean_accs >= np.max(accs_hist[0]):\n",
    "                best_acc_layers = []\n",
    "                for ii in layers_update:\n",
    "                    best_acc_layers.append(ii.detach().clone())\n",
    "        else:\n",
    "            # save best test\n",
    "            if np.max(test_acc) >= np.max(accs_hist[1]):\n",
    "                best_acc_layers = []\n",
    "                for ii in layers_update:\n",
    "                    best_acc_layers.append(ii.detach().clone())\n",
    "        \n",
    "        print(\"Epoch {}/{} done. Train accuracy: {:.2f}%, Test accuracy: {:.2f}%.\".format(e+1,nb_epochs, accs_hist[0][-1]*100, accs_hist[1][-1]*100))\n",
    "    \n",
    "\n",
    "    return loss_hist, accs_hist, best_acc_layers, ttc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb74d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(params, ds_train, ds_test, epochs=epochs):\n",
    "    \n",
    "    global nb_input_copies\n",
    "    nb_input_copies = params['nb_input_copies']  # Num of spiking neurons used to encode each channel \n",
    "\n",
    "    # Network parameters\n",
    "    global nb_inputs\n",
    "    nb_inputs  = nb_channels*nb_input_copies\n",
    "    global nb_hidden\n",
    "    nb_hidden  = 450\n",
    "    global nb_outputs\n",
    "    nb_outputs = len(np.unique(labels))+1\n",
    "    global nb_steps\n",
    "    nb_steps = data_steps\n",
    "\n",
    "    tau_mem = params['tau_mem'] # ms\n",
    "    tau_syn = tau_mem/params['tau_ratio']\n",
    "    \n",
    "    global alpha\n",
    "    alpha   = float(np.exp(-time_step/tau_syn))\n",
    "    global beta\n",
    "    beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "    fwd_weight_scale = params['fwd_weight_scale']\n",
    "    rec_weight_scale = params['weight_scale_factor']*fwd_weight_scale\n",
    "\n",
    "    # Spiking network\n",
    "    layers = []\n",
    "    w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w1, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_inputs))\n",
    "    layers.append(w1)\n",
    "\n",
    "    w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w2, mean=0.0, std=fwd_weight_scale/np.sqrt(nb_hidden))\n",
    "    layers.append(w2)\n",
    "    \n",
    "    v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(v1, mean=0.0, std=rec_weight_scale/np.sqrt(nb_hidden))\n",
    "    layers.append(v1)\n",
    "\n",
    "    layers_init = []\n",
    "    for ii in layers:\n",
    "        layers_init.append(ii.detach().clone())\n",
    "\n",
    "    opt_parameters = [w1, w2, v1]\n",
    "\n",
    "    loss_hist, accs_hist, best_layers, _ = train(params, ds_train, nb_epochs=epochs, opt_parameters=opt_parameters, layers=layers, dataset_test=ds_test)\n",
    "\n",
    "    # best training and corresponding test\n",
    "    acc_best_train = np.max(accs_hist[0])\n",
    "    acc_best_train = acc_best_train*100\n",
    "    idx_best_train = np.argmax(accs_hist[0]) \n",
    "    acc_test_at_best_train = accs_hist[1][idx_best_train]*100\n",
    "\n",
    "    # best test and corresponding training\n",
    "    acc_best_test = np.max(accs_hist[1])\n",
    "    acc_best_test = acc_best_test*100\n",
    "    idx_best_test = np.argmax(accs_hist[1])\n",
    "    acc_train_at_best_test = accs_hist[0][idx_best_test]*100\n",
    "\n",
    "    print(\"Final results: \\n\")\n",
    "    print(\"Best training accuracy: {:.2f}% and according test accuracy: {:.2f}% at epoch: {}\".format(acc_best_train, acc_test_at_best_train, idx_best_train+1)) # only from training\n",
    "    print(\"Best test accuracy: {:.2f}% and according train accuracy: {:.2f}% at epoch: {}\".format(acc_best_test, acc_train_at_best_test, idx_best_test+1)) # only from training\n",
    "\n",
    "    return loss_hist, accs_hist, best_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f786ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy(params, dataset, layers=None, early=False):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=128,\n",
    "                           shuffle=False, num_workers=2)\n",
    "    accs = []\n",
    "    multi_accs = []\n",
    "    ttc = None\n",
    "\n",
    "    for x_local, y_local in generator:\n",
    "        x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "        if layers == None:\n",
    "            layers = [w1,w2,v1]\n",
    "            output, others, _= run_snn(x_local,layers)\n",
    "        else:\n",
    "            output, others, _ = run_snn(x_local,layers)\n",
    "\n",
    "        m = torch.sum(others[-1],1) # sum over time\n",
    "        _, am = torch.max(m, 1)     # argmax over output units\n",
    "        # compare to labels\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "        accs.append(tmp)\n",
    "\n",
    "        if early:\n",
    "            accs_early = []\n",
    "            for t in range(output.shape[1]-1):\n",
    "                m_early = torch.sum(others[-1][:,:t+1,:],1) # sum over time\n",
    "                _, am_early = torch.max(m_early, 1)         # argmax over output units\n",
    "                # compare to labels\n",
    "                tmp_early = np.mean((y_local == am_early).detach().cpu().numpy())\n",
    "                accs_early.append(tmp_early)\n",
    "            multi_accs.append(accs_early)\n",
    "    \n",
    "    if early:\n",
    "        mean_multi = np.mean(multi_accs, axis=0)\n",
    "        if np.max(mean_multi) > mean_multi[-1]:\n",
    "            if mean_multi[-2]==mean_multi[-1]:\n",
    "                flattening = []\n",
    "                for ii in range(len(mean_multi)-2,1,-1):\n",
    "                    if mean_multi[ii] != mean_multi[ii-1]:\n",
    "                        flattening.append(ii)\n",
    "                # time to classify\n",
    "                ttc = time[flattening[0]]\n",
    "            else:\n",
    "                # time to classify\n",
    "                ttc = time[-1]\n",
    "        else:\n",
    "            # time to classify\n",
    "            ttc = time[np.argmax(mean_multi)]\n",
    "\n",
    "    return np.mean(accs), ttc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe0a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(dataset, save, layers=None, labels=letters):\n",
    "    \n",
    "    generator = DataLoader(dataset, batch_size=128,\n",
    "                           shuffle=False, num_workers=2)\n",
    "    accs = []\n",
    "    trues = []\n",
    "    preds = []\n",
    "    for x_local, y_local in generator:\n",
    "        x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "        if layers == None:\n",
    "            layers = [w1, w2, v1]\n",
    "            output, others, _= run_snn(x_local, layers)\n",
    "        else:\n",
    "            output, others, _= run_snn(x_local, layers)\n",
    "\n",
    "        m = torch.sum(others[-1],1) # sum over time\n",
    "        _, am = torch.max(m, 1)     # argmax over output units\n",
    "        # compare to labels\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "        accs.append(tmp)\n",
    "        trues.extend(y_local.detach().cpu().numpy())\n",
    "        preds.extend(am.detach().cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(trues, preds, normalize='true')\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "    plt.figure(figsize=(12,9))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.1g',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.xticks(rotation=0)\n",
    "    if save:\n",
    "        plt.savefig(\"../plots/rsnn_thr_\" + str(threshold) + \"_cm.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78301adf",
   "metadata": {},
   "source": [
    "Load braille data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a7cdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir_data = './data/'\n",
    "file_type = 'data_braille_letters_izhi'\n",
    "file_type_orig = 'data_braille_letters_th1'\n",
    "file_name = file_dir_data + file_type\n",
    "\n",
    "file_dir_params = '../parameters/'\n",
    "param_filename = 'parameters_th1.txt'\n",
    "file_name_parameters = file_dir_params + param_filename\n",
    "params = {}\n",
    "with open(file_name_parameters) as file:\n",
    " for line in file:\n",
    "    (key, value) = line.split()\n",
    "    if key == 'time_bin_size' or key == 'nb_input_copies':\n",
    "        params[key] = int(value)\n",
    "    else:\n",
    "        params[key] = np.double(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faabd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements \n",
    "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid \n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = params['scale']\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which \n",
    "        we need to later backpropagate our error signals. To achieve this we use the \n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the \n",
    "        surrogate gradient of the loss with respect to the input. \n",
    "        Here we use the normalized negative part of a fast sigmoid \n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21104c",
   "metadata": {},
   "source": [
    "### Train and test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9869ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test, labels, nb_channels, data_steps = load_and_extract(params, file_name, letter_written=letters)\n",
    "\n",
    "loss_hist, acc_hist, best_layers = build_and_train(params, ds_train, ds_test, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa7b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(acc_hist[0])+1),100*np.array(acc_hist[0]), color='blue')\n",
    "plt.plot(range(1,len(acc_hist[1])+1),100*np.array(acc_hist[1]), color='orange')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend([\"Training\",\"Test\"], loc='lower right')\n",
    "if save_fig:\n",
    "    plt.savefig(\"../plots/rsnn_thr_\"+str(threshold)+\"_acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e2880",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix(ds_test, layers=best_layers, save=save_fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
